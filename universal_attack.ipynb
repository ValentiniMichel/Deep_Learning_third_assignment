{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 15:39:09.350910: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-26 15:39:09.390649: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-26 15:39:09.391360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 15:39:09.913870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the FashionMNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FashionMNIST dataset is loaded using the fashion_mnist.load_data() function. The dataset is then preprocessed by converting the pixel values to floating-point numbers and normalizing them to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 15:39:11.027056: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 2s 3ms/step - loss: 0.5714 - accuracy: 0.8059 - val_loss: 0.4398 - val_accuracy: 0.8435\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.4162 - accuracy: 0.8529 - val_loss: 0.3991 - val_accuracy: 0.8583\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3706 - accuracy: 0.8693 - val_loss: 0.3847 - val_accuracy: 0.8615\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3519 - accuracy: 0.8739 - val_loss: 0.3594 - val_accuracy: 0.8677\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3294 - accuracy: 0.8825 - val_loss: 0.3566 - val_accuracy: 0.8703\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3133 - accuracy: 0.8873 - val_loss: 0.3453 - val_accuracy: 0.8748\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3036 - accuracy: 0.8896 - val_loss: 0.3387 - val_accuracy: 0.8802\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2884 - accuracy: 0.8953 - val_loss: 0.3347 - val_accuracy: 0.8783\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.8992 - val_loss: 0.3359 - val_accuracy: 0.8815\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2729 - accuracy: 0.8999 - val_loss: 0.3488 - val_accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f189beae0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Build and train a deep learning model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation=\"relu\"),   #reshape the input \n",
    "    layers.Dense(10, activation=\"softmax\")  #reshape the output. for 10 classes in FashionMNIST\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential model is created using Keras, consisting of a flatten layer to reshape the input, a dense layer with 128 units and ReLU activation, and a dense output layer with 10 units (corresponding to the 10 classes in FashionMNIST) and softmax activation. The model is then compiled with the Adam optimizer and sparse categorical cross-entropy loss function. Finally, it is trained on the training data with a batch size of 128 and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial examples for multiple inputs\n",
    "epsilon = 0.01\n",
    "\n",
    "def generate_adversarial_examples(model, inputs, epsilon=0.01):\n",
    "    # Create a copy of the inputs\n",
    "    adversarial_examples = np.copy(inputs)\n",
    "\n",
    "    # Convert inputs to TensorFlow tensors\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    adversarial_examples = tf.convert_to_tensor(adversarial_examples)\n",
    "\n",
    "    # Compute the gradients of the loss with respect to the inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(adversarial_examples)\n",
    "        predictions = model(adversarial_examples)\n",
    "        loss = keras.losses.sparse_categorical_crossentropy(y_test, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, adversarial_examples)\n",
    "\n",
    "    # Apply perturbations to the inputs based on the gradients\n",
    "    perturbations = epsilon * tf.sign(gradients)\n",
    "    adversarial_examples += perturbations\n",
    "\n",
    "    return adversarial_examples.numpy()\n",
    "\n",
    "# Generate adversarial examples for multiple inputs\n",
    "x_test_adv = generate_adversarial_examples(model, x_test, epsilon)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function generate_adversarial_examples is defined to generate adversarial examples for multiple inputs. The function takes a deep learning model, the inputs, and an epsilon value (perturbation magnitude) as input. Adversarial examples are generated by applying perturbations to the inputs based on the gradients of the loss with respect to the inputs. TensorFlow's GradientTape is used to record the operations and compute the gradients. The function returns the adversarial examples as a NumPy array. The adversarial examples are generated using the generate_adversarial_examples function for the test inputs, and they are stored in the x_test_adv variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 897us/step\n",
      "Success rate on the original model: 0.1323\n",
      "Epoch 1/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.7930 - val_loss: 0.4584 - val_accuracy: 0.8422\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.4351 - accuracy: 0.8474 - val_loss: 0.4030 - val_accuracy: 0.8530\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3957 - accuracy: 0.8630 - val_loss: 0.3910 - val_accuracy: 0.8605\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3724 - accuracy: 0.8685 - val_loss: 0.3915 - val_accuracy: 0.8610\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3524 - accuracy: 0.8741 - val_loss: 0.3733 - val_accuracy: 0.8652\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3357 - accuracy: 0.8795 - val_loss: 0.3541 - val_accuracy: 0.8738\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8835 - val_loss: 0.3623 - val_accuracy: 0.8708\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3122 - accuracy: 0.8886 - val_loss: 0.3461 - val_accuracy: 0.8778\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3024 - accuracy: 0.8917 - val_loss: 0.3436 - val_accuracy: 0.8782\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2955 - accuracy: 0.8937 - val_loss: 0.3565 - val_accuracy: 0.8745\n",
      "313/313 [==============================] - 0s 999us/step\n",
      "313/313 [==============================] - 0s 938us/step\n",
      "Success rate on the new model: 0.0968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate the success rate of the universal attack\n",
    "def evaluate_success_rate(model, inputs, targets):\n",
    "    # Get model predictions for the inputs\n",
    "    original_predictions = np.argmax(model.predict(inputs), axis=1)\n",
    "    adversarial_predictions = np.argmax(model.predict(x_test_adv), axis=1)\n",
    "\n",
    "    # Count the successful attacks\n",
    "    successful_attacks = np.sum(original_predictions != adversarial_predictions)\n",
    "\n",
    "    # Calculate the success rate\n",
    "    success_rate = successful_attacks / len(inputs)\n",
    "\n",
    "    return success_rate\n",
    "\n",
    "# Evaluate the success rate of the universal attack on the original model\n",
    "success_rate_original_model = evaluate_success_rate(model, x_test, y_test)\n",
    "print(\"Success rate on the original model:\", success_rate_original_model)\n",
    "\n",
    "# Create a new deep learning model with a different architecture\n",
    "new_model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "new_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "new_model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the success rate of the universal attack on the new model\n",
    "success_rate_new_model = evaluate_success_rate(new_model, x_test, y_test)\n",
    "print(\"Success rate on the new model:\", success_rate_new_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `evaluate_success_rate` is defined to evaluate the success rate of the universal attack. The function takes a model, inputs, and targets as input. It makes predictions on the original inputs and the adversarial examples generated earlier. Then, it compares the predicted labels of the original inputs with the adversarial examples to count the number of successful attacks. Finally, it calculates the success rate as the ratio of successful attacks to the total number of inputs.\n",
    "\n",
    "The success rate of the universal attack is evaluated on the original model (`model`) and a new model (`new_model`) with a different architecture. The success rates for both models are computed using the `evaluate_success_rate` function and printed.\n",
    "\n",
    "By comparing the success rates on different models, you can evaluate the transferability of the universal attack to other deep learning architectures. If the success rate remains high across different models, it suggests that the attack is more universal and not specific to a particular architecture.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
